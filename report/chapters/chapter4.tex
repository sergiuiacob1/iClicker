\chapter{Date de antrenament}
\label{chapter3}
\section{Colectarea și salvarea datelor}
% \footnote{Expresia este ``garbage in, garbage out'', iar o expresie românească similară ar fi ``semeni vânt, culegi furtună''}
Datele de antrenament sunt extrem de importante.
O expresie populară (\emph{garbage in, garbage out}) ne spune că dacă datele furnizate algorimilor de învățare automată sunt de proastă calitate, atunci așa va fi și performanța acelor algoritmi, sau mai bine spus a modelelor matematice și a generalizărilor create de aceștia.
Mulțimea de date trebuie să conțină varietate, în cazul de față însemnând poze cu fundaluri diferite, condiții de iluminare diverse ș.a.m.d.

Am lucrat cu imagini cu utilizatorul, capturate prin webcam.
Când colectarea datelor este gata, acestea sunt salvate sub forma unei ``sesiuni''.
Fiecare sesiune este definită de numărul imaginilor care au fost capturate, de rezoluția ecranului și rezoluția webcam-ului.
Fiecare sesiune conține, pentru fiecare imagine, poziția de pe ecran la care se uita utilizatorul în momentul capturării imaginii.

\begin{lstlisting}[language=Python, caption=Colectarea datelor]
def start_collecting(self, collection_type):
    dc_logger.info(f'Start collecting data in {collection_type} mode')
    WebcamCapturer.start_capturing()
    self.gui.start()
    dc_logger.info('DataCollectorGUI started')
    if collection_type == 'background':
        self.mouse_listener.start_listening()
        dc_logger.info('Mouse listener started')
    elif collection_type == 'active':
        threading.Thread(target=self.start_active_collection).start()
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Salvarea datelor]
def save_collected_data(self):
    if len(self.collected_data) == 0:
        return
    dc_logger.info(
        f'Acquiring lock for data collection. Locked = {self.collect_data_lock.locked()}')
    self.collect_data_lock.acquire()
    dc_logger.info('Lock acquired')
    session_no = self.get_session_number()
    dc_logger.info(f"Saving data for session_{session_no}")
    self.save_session_info(session_no)
    self.save_images_info(session_no)
    self.save_images(session_no)
    self.collect_data_lock.release()
    self.collected_data = []
    dc_logger.info('Saving data done')
\end{lstlisting}

Toate datele folosite le-am plasat în directorul \emph{data}.
Acesta conține două subdirectoare (\emph{images} și \emph{sessions}) și un fișier \emph{sessions.json} cu informații despre sesiunile de colectare.
Imaginile sunt denumite sugestiv (\emph{NumărSesiune\_NumărImagine.png}) și sunt salvate în formatul \emph{PNG} (\emph{Portable Network Graphics}).
Directorul \emph{sessions} conține fișiere în formatul \emph{JSON} cu informațiile despre imaginile obținute în acea sesiune.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.49\textwidth]{sessions.png}
    \includegraphics[width=0.49\textwidth]{session_example.png}
    \caption{Fișiere cu informații despre sesiuni}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{data_structure.png}
    \caption{Structurarea datelor}
\end{figure}

\section{Procesarea datelor}
Din imaginile colecționate am extras fie doar fața acestuia din imagine, fie doar un ochi, fie o porțiune dreptunghiulară în care se regăsesc ambii ochi.
Pentru fiecare experiment realizat, am menționat ce date au fost folosite și în ce mod au fost procesate acestea.

\subsection{Avantajele unei preprocesări}
Procesarea datelor joacă un rol important în soluționarea unei probleme de învățare automată.
În primul rând, este  să eliminăm orice fel de ``zgomot'' sau informație neesențială din mulțimea noastră de date.
Apoi, putem deriva alte informații din datele inițiale, neprelucrate, informații care pot aduce un plus de performanță în soluția finală a problemei.

În cadrul unui experiment realizat m-am confruntat cu o situație care a dovedit chiar necesitatea (nu doar avantajul) de a folosi procesarea de date.
În lipsa normalizării datelor, rețeaua a fost incapabilă să învețe, indiferent de timpul de antrenare pe care i l-am acordat.

\subsection{Folosirea exclusivă a ochilor}
\label{data-processing:eyes}
În unele experimente, care urmează a fi prezentate în capitolul următor, m-am axat pe extragerea ochilor din imagini.
Pentru asta, am făcut uz de două biblioteci Python: dlib și imutils.
Bazându-mă pe reperele faciale care au fost prezentate în introducere\ref{figure:facial-landmarks}, am extras doar porțiunile imaginilor care delimitează ochii.
Fiecare ochi a fost redimensionat la 60x30 pixeli apoi au fost uniți orizontal, formând o imagine alb-negru de dimensiune 120x30 pixeli.

\begin{lstlisting}[language=Python, caption=Extragerea ochilor dintr-o imagine]
def extract_eyes(cv2_image):
    """Returns a list of images that contain the eyes extracted from the original image.

    First result is the left eye, second result is the right eye."""
    global _face_detector, _face_predictor
    if _detectors_are_initialised() == False:
        _initialize_detectors()

    gray_image = Utils.convert_to_gray_image(cv2_image)
    rects = _face_detector(gray_image, 0)
    if len(rects) > 0:
        shape = _face_predictor(gray_image, rects[0])
        shape = face_utils.shape_to_np(shape)

        eyes = []
        for eye in ["left_eye", "right_eye"]:
            # get the points for the contour
            (eye_start, eye_end) = face_utils.FACIAL_LANDMARKS_IDXS[eye]
            contour = shape[eye_start:eye_end]
            # get the upper left point, lower right point for this eye
            start = [min(contour, key=lambda x: x[0])[0],
                     min(contour, key=lambda x: x[1])[1]]
            end = [max(contour, key=lambda x: x[0])[0],
                   max(contour, key=lambda x: x[1])[1]]
            # extract the current eye
            eyes.append(cv2_image[start[1]:end[1], start[0]:end[0]])
        return eyes

    return None
\end{lstlisting}

Pentru prezicerea zonei în care se uită utilizatorul ne interesează mai mult contrastul dintre pupilă și iris.
Pentru aceasta, am aplicat un prag binar (\emph{binary threshold}) imaginilor ochilor (care au fost convertite în prealabil în imagini gri) pentru a scoate în evidență poziția pupilei, relativ la întregul ochi.

Un \emph{binary threshold} simplu constă în a alege o constantă $C \in [0, 255]$ și, pentru o imagine gri $I$, creăm o nouă imagine $\hat{I}$ în care înlocuim valorile fiecărui pixel $p \in I$ astfel:
\begin{equation}
    \hat{p} =
    \begin{cases}
      0, & \text{dacă}\ p < C \\
      1, & \text{altfel}
    \end{cases}
  \end{equation}

Din păcate, această metodă nu a dat cele mai bune rezultate.
În funcție de claritatea imaginilor și de modul în care se reflectă lumina în ochi, imaginile alb-negru rezultate nu erau foarte consistente din punct de vedere al informației pe care o ofereau.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\textwidth]{eye_strip_extended.png}
    \includegraphics[width=0.49\textwidth]{simple_binary_threshold.png}
    \caption{Aplicarea unui \emph{binary threshold} simplu cu $C=127$}
\end{figure}

O soluție mai bună ar fi ca pentru fiecare pixel să ținem cont și de valorile pixelilor vecini deoarece ar fi o soluție mai robustă pentru imaginile care conțin mai multe variații de lumină.
O astfel de metodă este \emph{Gaussian Adaptive Thresholding} care, conform \cite{tutorial_py_thresholding}, calculează o sumă gaussiană ponderată a pixelilor vecini, din care apoi se scade valoarea $C$.
Metoda este net superioară celei simple și are rezultate mult mai bune.

Cu ajutorul bibliotecii OpenCV am aplicat mai întâi un filtru denumit \emph{Median Filter} cu scopul de a netezi imaginea (\emph{smoothing}).
Apoi am aplicat un prag de tipul \emph{Gaussian Adaptive Thresholding}.

\begin{lstlisting}[language=Python, caption=Aplicarea unui \emph{Gaussian Adaptive Thresholding}]
def get_binary_thresholded_image(cv2_image):
    img = convert_to_gray_image(cv2_image)
    img = cv2.medianBlur(img, 5)
    img = cv2.adaptiveThreshold(
        img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
    return img
\end{lstlisting}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\textwidth]{eye_strip_extended.png}
    \includegraphics[width=0.49\textwidth]{adaptive_thresholding_example.png}
    \caption{Thresholding Adaptiv Gaussian. Conturul ochilor este mai bine definit}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\textwidth]{eye_binary_threshold.png}
    \includegraphics[width=0.49\textwidth]{threshold_process.png}
    \caption{Obținerea de informații din imaginile ochilor}
\end{figure}

\subsection{Folosirea întregii fețe}
Următoarea idee a fost să folosesc fața completă a utilizatorului, apoi să o transmit unei rețele neuronale convoluționale.
Procesul de extragere a feței arată astfel:

\begin{figure}[h]
    \centering
    \includegraphics{extract_face_process.png}
    \caption{Extragerea feței}
    \label{fig_extracted_faces}
\end{figure}

Imaginea finală este în forma unui pătrat, conținând fața extrasă din imagine, convertită apoi in gri.
Este, de asemenea, normalizată, pentru a lua valori între $0$ și $1$.

Pentru a extrage fața am folosit biblioteca dlib.
Aceasta pune la dispoziție un detector facial care identifică un \emph{bounding box} în care se regăsește fața unei persoane.

\begin{lstlisting}[language=Python, caption=Extragerea feței dintr-o imagine]
def extract_face(cv2_image):
    """Returns the face part extracted from the image"""
    global _face_detector
    if _detectors_are_initialised() == False:
        _initialize_detectors()

    gray_image = Utils.convert_to_gray_image(cv2_image)
    rects = _face_detector(gray_image, 0)
    if len(rects) > 0:
        # only for the first face found
        (x, y, w, h) = face_utils.rect_to_bb(rects[0])
        return cv2_image[y:y+h, x:x+w]
    return None
\end{lstlisting}

\subsection{``Bandă oculară''}
Următoarea opțiune și cea care a rămas integrată în soluția finală a fost să folosesc ambii ochi, fără modificări adiționale.
Procesul de extragere a ochilor arată astfel:

\begin{figure}[h]
    \centering
    \includegraphics[width = \textwidth]{extract_eye_strip_process.png}
    \caption{Extragerea ``bandei oculare''}
    \label{fig_extracting_eye_strip}
\end{figure}

După ce am detectat fața, am extras ambii ochi, am mărit zona dreptunghiulară care încadrează ochii, pentru a avea mai multă informație, apoi am convertit imaginea în gri și am normalizat valorile pixelilor.
Zona care încadrează ochii a fost extinsă cu 20\% pe orizontală și cu 60\% pe verticală.

\begin{lstlisting}[language=Python, caption=Extragerea ``bandei oculare'' în Python 3.7]
def extract_eye_strip(cv2_image):
    """Returns a horizontal image containing the two eyes extracted from the image"""
    global _face_detector, _face_predictor
    if _detectors_are_initialised() == False:
        _initialize_detectors()

    gray_image = Utils.convert_to_gray_image(cv2_image)
    rects = _face_detector(gray_image, 0)
    if len(rects) > 0:
        # only for the first face found
        shape = _face_predictor(gray_image, rects[0])
        shape = face_utils.shape_to_np(shape)
        (left_eye_start,
         left_eye_end) = face_utils.FACIAL_LANDMARKS_IDXS["left_eye"]
        (right_eye_start,
            right_eye_end) = face_utils.FACIAL_LANDMARKS_IDXS["right_eye"]
        # get the contour
        start, end = min(left_eye_start, right_eye_start), max(
            left_eye_end, right_eye_end)
        strip = shape[start:end]
        # get the upper left point, lower right point
        start = [min(strip, key=lambda x: x[0])[0],
                 min(strip, key=lambda x: x[1])[1]]
        end = [max(strip, key=lambda x: x[0])[0],
               max(strip, key=lambda x: x[1])[1]]
        # go a little outside the bounding box, to capture more details
        distance = (end[0] - start[0], end[1] - start[1])
        # 20 percent more details on the X axis, 60% more details on the Y axis
        percents = [20, 60]
        for i in range(0, 2):
            start[i] -= int(percents[i]/100 * distance[i])
            end[i] += int(percents[i]/100 * distance[i])
        return cv2_image[start[1]:end[1], start[0]:end[0]]
    return None

\end{lstlisting}